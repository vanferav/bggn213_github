---
title: "mini-project"
author: "Vanesa Fernandez"
format: pdf
---

```{r}
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"

# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
```

```{r}
wisc.df <- read.csv("WisconsinCancer.csv") 
```

```{r}
head(wisc.df) ## The id and diagnosis columns will not be used for most of the following steps
```

```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]

head(wisc.data) 
```

```{r}
# Create diagnosis vector for later 
diagnosis <- as.factor(wisc.df$diagnosis) ## Factors are for categorical data and modeling, also for plotting.

head(diagnosis)
```

> Q1 How many rows/ subjects?

```{r}
nrow(wisc.df)
```

>Q2. How many M (cancer) B (healthy) patients?

```{r}
table(diagnosis)
```

> Q3. How many variables/features in the data are suffixed with _mean?

```{r}
colnames(wisc.data) ## We could use `colnames` and count manually but it could be time consuming
```

## Or we could use grep() ## To finding Pattern Matching
```{r}
length(grep("_mean", colnames(wisc.data), value = T))
```
## Principal Component Analysis

```{r}
# Check column means and standard deviations to determine if the data should be scaled. 

colMeans(wisc.data[,2:31])

apply(wisc.data,2,sd)
```

We want to scale our data before PCA by setting the `scale=TRUE` argument! 

```{r}
wisc.pr <- prcomp(wisc.data[,2:31], scale = TRUE)
```
 
 How much variance captured in each PC?
```{r}
x <- summary (wisc.pr)
x$importance
```

```{r}
plot(x$importance[2,], typ="b")
```

```{r}
#biplot(wisc.pr)
```

```{r}
attributes(wisc.pr)
```

```{r}
head(wisc.pr$x)
```

My main PC result figure 
```{r}
plot(wisc.pr$x, col=diagnosis)
```

```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```

 Variance explained
 
```{r}
# Calculate variance of each component
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
 
```{r}
# Variance explained by each principal component: pve
pve <- pr.var / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```
Communicating PCA results
## The loadings, represented as vectors, explain the mapping from the original features to the principal components. The principal components are naturally ordered from the most variance explained to the least variance explained.

>Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC

```{r}
wisc.pr$rotation[,1]["concave.points_mean"]
```

Hierarchical clustering

##Try to cluster the `wisc.data`.
```{r}
km <- kmeans(wisc.data[,2:31], centers = 2)
table(km$cluster)
```

```{r}
d <- dist(wisc.data)
hc <- hclust (d)
plot(hc)
```


##Cluster in PC space
In other words, use my PCA results as a basis of clustering. 

```{r}
d <- dist(wisc.pr$x[,1:3])
hc <- hclust(d, method = "ward.D2")
plot (hc)
```

Cut this tree to yield 2 groups / clusters

```{r}
grps <- cutree (hc, k=2)
table(grps)
```

Compare to my expert M and B `diagnosis`
```{r}
table(diagnosis)
```

```{r}
table(diagnosis, grps)
```


```{r}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data[,2:31])
```

## Calculate the (Euclidean) distances between all pairs of observations in the new scaled dataset and assign the result to data.dist.

```{r}
data.dist <- dist(data.scaled, method = "euclidean")
```

## Create a hierarchical clustering model using complete linkage. Manually specify the method argument to hclust() and assign the results to wisc.hclust.

```{r}
wisc.hclust <- hclust(data.dist)
```

Results of hierarchical clustering
> Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?

```{r}
plot (wisc.hclust)
abline(h=19, col="red", lty=2)
```

Selecting number of clusters
## Use cutree() to cut the tree so that it has 4 clusters. Assign the output to the variable wisc.hclust.clusters.
```{r}
wisc.hclust.clusters <- cutree (wisc.hclust, k=4)
head(wisc.hclust.clusters)
```

##We can use the table() function to compare the cluster membership to the actual diagnoses.

```{r}
table(wisc.hclust.clusters, diagnosis)
```

Using different methods
> Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.

## I like the "complete" method because the height and hierarchies look more clear, but detailed. 
```{r}
wisc.hclust <- hclust(data.dist, "complete")
```

```{r}
plot (wisc.hclust)
abline(h=19, col="red", lty=2)
```


```{r}
loadings <- wisc.pr$rotation

ggplot(loadings) +
  aes(abs(PC1), reorder(rownames(loadings), -PC1)) + geom_col()

```

